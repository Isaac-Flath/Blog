{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc73ca5",
   "metadata": {},
   "source": [
    "# CycleGAN Walk Through\n",
    "> A walkthrough of key components to a pytorch CycleGAN implementation.\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Isaac Flath\n",
    "- categories: [Computer Vision, GAN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192431c9",
   "metadata": {},
   "source": [
    "In this post I will build on my [previous](https://isaac-flath.github.io/fastblog/computer%20vision/gan/2021/02/20/GANBasics.html) [posts](https://isaac-flath.github.io/fastblog/computer%20vision/gan/2021/03/01/StyleGanComponents.html) on GANs and talk about CycleGAN.  \n",
    "\n",
    "In StyleGAN, we took noise and generated an image realistic enough to fool the discriminator.  In CycleGAN we take an image and modify it to a different class to make that modified image realistic enough to fool the discriminator into believing it's that class.\n",
    "\n",
    "I am going to walk through a great [Pytorch CycleGAN implementation](https://github.com/aitorzip/PyTorch-CycleGAN) and explain what the pieces are doing in plain english so anyone can understand the important bits without diving through lots of code or reading an academic paper.\n",
    "\n",
    "Before we jump in - here's the three most important pieces to CycleGAN to understand if you want to skip to the most crucial bits.  I labeled the key sections in the Table of Contents for you. \n",
    "\n",
    "1. There are 2 generators and 2 discriminators being trained.  4 total models!\n",
    "1. The Generator Loss function has 3 components: Adverserial Loss, Cycle Loss, and Identity loss.  Understanding these is key.\n",
    "1. The Discriminator predicts real or fake for lots of different chunks of the image, not just 1 prediction for the whole image.\n",
    "\n",
    "These will be explained in detail as we go so don't worry if that doesn't completely make sense just yet.  It will :)\n",
    "\n",
    ">Note: As you read if you like the post and want regular updates when I write new posts, [follow me on twitter](https://twitter.com/isaac_flath).\n",
    "\n",
    "So let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2295e0c1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastcore.basics import *\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557b46e",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "\n",
    "This implementation of CycleGAN is using basic transforms that are not unique to CycleGAN so I won't be diving into detail on those in this post.  Please post a comment or message me on twitter if you have questions or want a post that talks in more detail on transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6191f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaacflath/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transforms_train = [ transforms.Resize(int(256*1.12), Image.BICUBIC), \n",
    "                     transforms.RandomCrop(256), \n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
    "transforms_test = [ transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978822ac",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8567d",
   "metadata": {},
   "source": [
    "The dataset isn't anything special other than a batch being images from both classes (A and B).  This is a standard pytorch dataloader so I won't cover what's going on in this post, but there is a [great tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) if you would like to understand this more.\n",
    "\n",
    "There are 2 key things to notice here:\n",
    "+ A batch is a dictionary of images from class A and images from class B.\n",
    "+ This example would be style transfer between summer and winter pictures (at Yosemite)\n",
    "\n",
    ">Note: I have added a show_batch method to the dataloader.  This is an idea I took from fastai and it I highly reccomend making sure you have a very easy way to visualize anything you are working with.  It will save you lots of time if you get that set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967b8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, f'{mode}A') + '/*.*'))[:50]\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, f'{mode}B') + '/*.*'))[:50]\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "\n",
    "        if self.unaligned: item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n",
    "        else:              item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
    "\n",
    "        return {'A': item_A, 'B': item_B}\n",
    "\n",
    "    def __len__(self): return max(len(self.files_A), len(self.files_B))\n",
    "    \n",
    "    def show_batch(self,sets=2, cols=3):\n",
    "        idxs = random.sample(range(self.__len__()), cols*2*sets)        \n",
    "        fig, ax = plt.subplots(2*sets, cols,figsize=(4*cols,4*2*sets))\n",
    "        \n",
    "        for r in range(sets):\n",
    "            for col in range(0,cols):\n",
    "                row=r*2\n",
    "                num = (row * cols + col)\n",
    "                x = self[idxs[num]]['A'].permute(1,2,0)\n",
    "                ax[row,col].imshow(0.5 * (x+1.)); ax[row,col].axis('off')\n",
    "\n",
    "                row=row+1\n",
    "                num = (row * cols + col)\n",
    "                x = self[idxs[num]]['B'].permute(1,2,0)\n",
    "                ax[row,col].imshow(0.5*(x+1.)); ax[row,col].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f0251",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We have discriminators and generators - let's look briefly at what they output, then dive into some details.\n",
    "\n",
    "+ The discriminator outputs a bunch of predictions as to whether different portions of an image is a real image of that class or a fake image of that class\n",
    "+ The generator is taking a real image and converting it to the other class.  For example a picture of a lake in the Summer goes in and a picture of that same lake in the winter should come out (maybe adding snow for example).\n",
    "\n",
    ">Note:  I am assuming you have a general understanding of what the role of a discriminator vs generator is and how they train together.  If you need a refresher [read this](https://isaac-flath.github.io/fastblog/computer%20vision/gan/2021/02/20/GANBasics.html##How-Does-it-Work?) section of my GAN Basics blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecc24c",
   "metadata": {},
   "source": [
    "#### Discriminator - Key 1\n",
    "\n",
    "The most important thing to understand about any model is what it's predicting.  Let's take a look at the last thing that is done before it's output and understand that first.\n",
    "\n",
    "+ **avg_pool2d:** At the end there's average pooling, which is just calculated averages in different patches of the feature map.  So really what we are predicting is not whether the image is real or fake, but splitting the image into lots of pieces and determining if each piece individually is real or fake.\n",
    "\n",
    "This gives the generator much more information to be able to optimize to.  Predicting whether an image is real or fake is much easier than generating a whole image - so we want to help the generator as much as possible.\n",
    "\n",
    "If you think about this intuitively - it makes perfect sense.  If you were trying to draw a realistic still life and you showed it to an expert artist for feedback what kind of feedback would you like?  Would you like them to tell you it looks real or looks fake and leave it at that?  Or would you get more out of them breaking the painting into pieces, telling you what portions are realistic and what portions need more work?  Of course, the latter is more helpful so that's what we predict for the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdb19c",
   "metadata": {},
   "source": [
    "The rest of the discriminator is nothing special but let's dive in a bit to prove that.  Here's the components:\n",
    "\n",
    "+ **Conv2d:**  When working with images convolutions are very common\n",
    "+ **LeakyReLU:**  While ReLU is more common, Leaky ReLU is used.  We don't want the model to get stuck in a 'no-training' zone that exists with ReLU.  GANs are harder to train well because of the additional complexity of the adversarial model so LeakyReLU works better on GANs generall.\n",
    "+ **InstanceNorm2d:** BatchNorm is more common, but this is just a small tweak from that.  If you think about the different meanings of the word \"Instance\" vs \"Batch\" you make be able to guess what the difference is.  In short BatchNorm is normalizing across the entire batch (computing 1 mean/std).  InstanceNorm is normalizing over the individual image (instance), so you have a mean and std for each image.\n",
    "\n",
    "\n",
    ">Note: If you think through the impact of batch vs Instance normalization you may realize that with BatchNorm the training for a particular image is effected by which images happen to be in the same batch.  This is because the mean and standard deviation are calculated across the entire batch, rather than for that image alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e1fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # A bunch of convolutions one after another\n",
    "        model = [   nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(128), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "                    nn.InstanceNorm2d(256), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        model += [  nn.Conv2d(256, 512, 4, padding=1),\n",
    "                    nn.InstanceNorm2d(512), \n",
    "                    nn.LeakyReLU(0.2, inplace=True) ]\n",
    "\n",
    "        # FCN classification layer\n",
    "        model += [nn.Conv2d(512, 1, 4, padding=1)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.model(x)\n",
    "        # Average pooling and flatten\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a6783",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "\n",
    "The Generator is what generates the image.  It's got a lot of the same components as other Neural Networks.  Let's talk about the components.\n",
    "\n",
    "Let's break this apart and talk about each piece briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181896b7",
   "metadata": {},
   "source": [
    "###### Initial Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f79656",
   "metadata": {},
   "source": [
    "So this is the code from the implementiation for the first bit of the generator (I cut off the rest to be shown later).  Let's understand this first.\n",
    "\n",
    "We see all the same components we say avove.  `Conv2d` is doing convolutions (big 7x7 ones), we also have InstanceNorm like we saw in the Discriminator (discussed above), and a common activation function `ReLU`.  \n",
    "\n",
    "The new thing is this ReflectionPad2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f65bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block       \n",
    "        model = [   nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(input_nc, 64, 7),\n",
    "                    nn.InstanceNorm2d(64),\n",
    "                    nn.ReLU(inplace=True) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d36d7",
   "metadata": {},
   "source": [
    "So what is ReflectionPad2d?  First, let's look at what a convolution does.  The blue in the gif below is the image, the white squares are padding.  Normally they're padded with nothing like in the illustration.  What ReflectionPad does is pads that with a reflection of the image instead.  In other words, we are using the pixels values of pixels on the edge to pad instead of just a pure white or pure black pixel.\n",
    "\n",
    ">Note: Fore more on convolutions [go here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53).  The gif below comes from that guide by [Sumit Saha](https://medium.com/@_sumitsaha_) and the guide contains a lot of other create information.\n",
    "\n",
    "![](https://miro.medium.com/max/395/1*1VJDP6qDY9-ExTuQVEOlVg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bba277",
   "metadata": {},
   "source": [
    ">Note: Credit for Visualization: Vincent Dumoulin, Francesco Visin - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b41cf",
   "metadata": {},
   "source": [
    "###### Downsampling\n",
    "\n",
    "We then go through several downsampling layers.  A 3x3 convolution with stride 2 will result in a smaller feature map, which is exactly what we are doing to cause the downsampling.  It's all the usual suspects through: `convolutions`, `InstanceNorms`, and `ReLUs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1967c4e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m out_features \u001b[38;5;241m=\u001b[39m in_features\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     model \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [  nn\u001b[38;5;241m.\u001b[39mConv2d(in_features, out_features, \u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      6\u001b[0m                 nn\u001b[38;5;241m.\u001b[39mInstanceNorm2d(out_features),\n\u001b[1;32m      7\u001b[0m                 nn\u001b[38;5;241m.\u001b[39mReLU(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) ]\n\u001b[1;32m      8\u001b[0m     in_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m      9\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m in_features\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "        # Downsampling\n",
    "        in_features = 64\n",
    "        out_features = in_features*2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a49abe",
   "metadata": {},
   "source": [
    "###### Residual Blocks\n",
    "\n",
    "Next we go through some residual blocks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Residual blocks\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66831e",
   "metadata": {},
   "source": [
    "When we look at residual blocks again, it's all the same components in slightly different configurations as above.  We have `ReflectionPad`, `Convolutions`, `InstanceNorm`, and `ReLUs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ec3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [  nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.ReflectionPad2d(1),\n",
    "                        nn.Conv2d(in_features, in_features, 3),\n",
    "                        nn.InstanceNorm2d(in_features)  ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6137fca",
   "metadata": {},
   "source": [
    "###### Upsampling\n",
    "\n",
    "Next is upsampling.  There is a new component here which `ConvTranspose`.  Let's take a look at what that is exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329aabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "        out_features = in_features//2\n",
    "        for _ in range(2):\n",
    "            model += [  nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                        nn.InstanceNorm2d(out_features),\n",
    "                        nn.ReLU(inplace=True) ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features//2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e945f55",
   "metadata": {},
   "source": [
    "So what is this?  Well essentially it's a normal convolution that upsamples by creating padding between cells.  Here's a visual that shows what that looks like.\n",
    "\n",
    "![](https://i.stack.imgur.com/f2RiP.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca859c2",
   "metadata": {},
   "source": [
    ">Note: Credit for Visualization: Vincent Dumoulin, Francesco Visin - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540020f7",
   "metadata": {},
   "source": [
    "######  Output Layer\n",
    "\n",
    "Finally we have out output layer with a `Tanh` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        model += [  nn.ReflectionPad2d(3),\n",
    "                    nn.Conv2d(64, output_nc, 7),\n",
    "                    nn.Tanh() ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526b1f8",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a225c",
   "metadata": {},
   "source": [
    "The model using the Adam Optimizer with a scheduler.  I am going to skip over that and look at the most interesting and important part of CycleGAN.  The loss functions!\n",
    "\n",
    "If you recall, we have both generators and Discriminators.  So we need a loss function for each.  Let's look at each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ad5e3",
   "metadata": {},
   "source": [
    "#### Discriminator Loss\n",
    "\n",
    "The discriminator loss is a standard adversarial loss.  Let's think through what we would need:\n",
    "\n",
    "+ Real images of a class (ie Summer Yosimite pictures)\n",
    "+ Fake images of a class (ie generated Summer Yosimite pictures)\n",
    "+ Discriminator predictions for whether each section of the image is real or fake\n",
    "\n",
    "So let's say we generated the images with our generator and then we took the real images from our batch, the fake generated images, and ran that through our disciminator.  Once we have that we use Mean Squared Error as the loss function.  \n",
    "\n",
    "Let's see how this works.  Everything is duplicated because we have 2 discriminators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502d716",
   "metadata": {},
   "source": [
    "**Discriminator 1:** Is each section of this Class A image real or fake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ef3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "        pred_real = netD_A(real_A) # Predict whether real image is real or fake\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real) \n",
    "\n",
    "        pred_fake = netD_A(fake_A.detach()) # Predict whether fake image is real or fake\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake)\n",
    "\n",
    "        loss_D_A = (loss_D_real + loss_D_fake)*0.5 # Total loss\n",
    "        loss_D_A.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdbf4a",
   "metadata": {},
   "source": [
    "**Discriminator 2:** Is each section of this Class B image real or fake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e94010",
   "metadata": {},
   "outputs": [],
   "source": [
    "        pred_real = netD_B(real_B) # Predict whether real image is real or fake\n",
    "        loss_D_real = criterion_GAN(pred_real, target_real) \n",
    "\n",
    "        pred_fake = netD_B(fake_B.detach()) # Predict whether fake image is real or fake\n",
    "        loss_D_fake = criterion_GAN(pred_fake, target_fake) \n",
    "\n",
    "        loss_D_B = (loss_D_real + loss_D_fake)*0.5 # Total loss\n",
    "        loss_D_B.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde2e27",
   "metadata": {},
   "source": [
    "#### Generator Loss - Key 2\n",
    "\n",
    "The generator loss is the key to CycleGAN and it has three main parts to it.\n",
    "\n",
    "1. Adverserial Loss:  This is standard MSE Loss.  This is the most straightforward loss.  \n",
    "1. Identity Loss: This is L1Loss (pixel by pixel comparison to minimize the difference in pixel values).  If my generator is trained to take a Summer picture and turn it into a Winter picture and I give it winter picture is should do nothing (identity function).  The generator should look at the Winter Picture and determin that nothing needs to be done to make it a Winter picture as that's what it already is.  Identity loss is just trying this out and then comparing the input image with the output image.\n",
    "1. Cycle Loss:  This is where cycleGAN gets it's name.  L1 loss is just trying to minimize the difference in pixel values.  But how does it have images to compare when it's an unpaired dataset?\n",
    "    + Start with class A and run your Generator to create class B out of the Class A image\n",
    "    + Take that class B image that was just generated, and run it through the other generator to create a class A image\n",
    "    + If all you are doing is transferring styles you should get the exact same image back after the full cycle.  Those are the 2 images being compared.\n",
    "    \n",
    "These three components get added up for the loss function.  You can add weights to different portions to prioritize different aspects of the loss function.\n",
    "\n",
    "So how does this look all together?  You may notice everything is duplicated in the code.  That's because We have 2 generators:  \n",
    "+ Class A -> Class B or Summer -> Winter\n",
    "+ Class B -> Class A or Winter -> Summer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d199ac",
   "metadata": {},
   "source": [
    "**Adverserial Loss:** Is it good enough to fool the discriminator?\n",
    "\n",
    "This is the most straightforward and is standard MSE loss.  The generator is optimizing to fool the Discriminator.  Specifically the loss is being calculated on the disciminators prediction on fake images and a 'truth label' saying it is a real image.  We know it's not actually a real image, but the discriminator wants us to think so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3150ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        fake_B = netG_A2B(real_A) # Generate class B from class A\n",
    "        pred_fake = netD_B(fake_B) # Discriminator predict is is real or fake\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake, target_real) # Is discriminator fooled?\n",
    "\n",
    "        fake_A = netG_B2A(real_B) # Generate class A from class B\n",
    "        pred_fake = netD_A(fake_A) # Discriminator predict is is real or fake\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake, target_real) # Is discriminator fooled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a7095",
   "metadata": {},
   "source": [
    "**Identity Loss:** Is it making the minimum changes needed?\n",
    "\n",
    "Identity loss is L1 loss (pixel by pixel comparison to minimize the difference in pixel values).  If my generator is trained to take a Summer picture and turn it into a Winter picture and I give it winter picture, it should do nothing (identity function).  The generator should look at the Winter Picture and determin that nothing needs to be done to make it a Winter picture as that's what it already is.  Identity loss is doing this exactly and comparing the input image with the output image.  Since it should change nothing we can calculate the loss as the difference between the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7736f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "        same_B = netG_A2B(real_B) # Generate class B from class B\n",
    "        loss_identity_B = criterion_identity(same_B, real_B)*5.0 # Pixel Diff\n",
    "\n",
    "        same_A = netG_B2A(real_A) # Generate class A from class A\n",
    "        loss_identity_A = criterion_identity(same_A, real_A)*5.0 # Pixel Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1c707",
   "metadata": {},
   "source": [
    "**Cycle loss:** Is it only changing style?\n",
    "\n",
    "This is where cycleGAN gets it's name.  Cycle Loss is also an L1 Loss function - let's take a look at what images it's comparing.  Here's the process:\n",
    "    + Start with a class A image and run your Generator to generate a class B image\n",
    "    + Take that generated class B image and run it through the other generator to create a class A image (full cycle)\n",
    "    + Compare pixels between that generated Class A image should be identical to the original Class A input image\n",
    "    + Repeat in the other direction\n",
    "   \n",
    "If the only thing being changed is style then the generated Class A image that went through the full cycle should be identical to the original input Class A image.  If however other things are getting changed, then you will have information loss and you the images will be different.  By minizing this pixel difference you are telling the model not to change the general content of the image, it can only change stylistic things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "        recovered_A = netG_B2A(fake_B) # Generate Class A from fake Class B\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)*10.0 # Pixel Diff\n",
    "\n",
    "        recovered_B = netG_A2B(fake_A) # Generate Class B from fake Class A\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)*10.0 # Pixel Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fe59a",
   "metadata": {},
   "source": [
    "**Total Generator Loss:** Sum them all up into 1 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Total loss\n",
    "        loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB # Add all these losses up\n",
    "        loss_G.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf1b19",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf22a6b",
   "metadata": {},
   "source": [
    "That's really the guts of it. You throw that with an optimizer and scheduler in a training loop and you are pretty close to done!  Check out the repository linked at the start of the repository for the full implementation with all the details.\n",
    "\n",
    "If you want regular updates when I write new posts, [follow me on twitter](https://twitter.com/isaac_flath)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857722d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}